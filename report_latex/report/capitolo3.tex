\chapter{More advanced models comparison}
\label{capitolo3}
\thispagestyle{empty}

\noindent Starting from the basic model presented in the previous section, for the purpose of this project activity we theorized and tested various approaches to solving the problem.

Lasso
Elastic net
Gaussian Probability Regression
Isotonic Regression

\section[First model]{First model: adding more features with Linear Regression}
In order to make a step forward in the development of the regression model, our first attempt was to simply check out the performances obtained by a simple model with a few more features than the basic ones. In the attempt to study the impact of different features on the results, our choice for the model was the simple \texttt{Linear Regression} (implemented in \texttt{scikit-learn}), applied on a set of 12 features.

\begin{lstlisting}[firstnumber=35]
# Create a training file with simple derived features

rows = 150000 
segments = int( np.floor(train.shape[0]) / rows) 

X_train = pd.DataFrame(index=range(segments), dtype=np.float64,
                       columns=['ave', 'std', 'max', 'min', 'mad', 'kurt', 'skew', 'median', 'q01', 'q05', 'q95', 'q99'])

y_train = pd.DataFrame(index=range(segments), dtype=np.float64,
                       columns=['time_to_failure'])
 
for segment in tqdm(range(segments)):
    seg = train.iloc[segment*rows:segment*rows+rows]
    

    x = seg['acoustic_data'] 
    y = seg['time_to_failure'].values[-1]
    
    y_train.loc[segment, 'time_to_failure'] = y
    
    X_train.loc[segment, 'ave'] = x.mean()
    X_train.loc[segment, 'std'] = x.std()
    X_train.loc[segment, 'max'] = x.max()
    X_train.loc[segment, 'min'] = x.min()
    X_train.loc[segment, 'mad'] = x.mad()
    X_train.loc[segment, 'kurt'] = kurtosis(x)
    X_train.loc[segment, 'skew'] = skew(x)
    X_train.loc[segment, 'median'] = x.median()
    X_train.loc[segment, 'q01'] = np.quantile(x, 0.01)
    X_train.loc[segment, 'q05'] = np.quantile(x, 0.05)
    X_train.loc[segment, 'q95'] = np.quantile(x, 0.95)
    X_train.loc[segment, 'q99'] = np.quantile(x, 0.99)
\end{lstlisting}

In particular, we chose quite a standard set of features to add to the first four: \texttt{MAD} returns the Mean Absolute Deviation on the values (its accuracy is closely related to the Mean Squared Error, or \texttt{MSE}); \textit{kurtosis} is a measure of the "tailedness" (or the shape) of the probability distribution of a real-valued random variable, calculated as the fourth standardized moment; \textit{skewness} is a measure of the asymmetry of the probability distribution of a real-valued random variable, calculated as the third standardized moment; the \textit{median} is the value separating the higher half from the lower half of the data; the \textit{q-th quantiles} are cut points dividing the range of a probability distribution into intervals with the same probability: \textit{x} is a \textit{q}-th quantile for a variable \textit{X} if \(Pr[X<x]\leq q\).

The computed score of the so constructed model improves, even if slightly, the results of the na{\"i}ve model, with a value of 2,251.

\bigbreak

At this point, out of curiosity we took a look at the predicted \texttt{time\textunderscore to\textunderscore failure} resulting from the test data, and we noticed that there was a considerable number of negative values, evidently wrong (it should be remembered that they represent the time between the current segment and the next laboratory earthquake, which cannot be negative quantities).

Based on that observation, and given that the nature of the data is approximately symmetrical (see figure \ref{fig:plot2}), we though about introducing a whole new set of features generated by the same computational functions applied on the absolute values of the dataset.

\begin{lstlisting}[firstnumber=39]
X_train = pd.DataFrame(index=range(segments), dtype=np.float64,
                       columns=['ave', 'std', 'max', 'min', 'mad', 'kurt', 'skew', 'median', 'q01', 'q05', 'q95', 'q99', 'abs_mean', 'abs_std', 'abs_max', 'abs_min', 'abs_mad', 'abs_kurt', 'abs_skew', 'abs_median', 'abs_q01', 'abs_q05', 'abs_q95', 'abs_q99'])
\end{lstlisting}

\begin{lstlisting}[firstnumber=66]
	[...]
    X_train.loc[segment, 'abs_mean'] = x.abs().mean()
    X_train.loc[segment, 'abs_std'] = x.abs().std()
    X_train.loc[segment, 'abs_max'] = x.abs().max()
    X_train.loc[segment, 'abs_min'] = x.abs().min()
    X_train.loc[segment, 'abs_mad'] = x.abs().mad()
    X_train.loc[segment, 'abs_kurt'] = kurtosis(x.abs())
    X_train.loc[segment, 'abs_skew'] = skew(x.abs())
    X_train.loc[segment, 'abs_median'] = x.abs().median()
    X_train.loc[segment, 'abs_q01'] = np.quantile(x.abs(), 0.01)
    X_train.loc[segment, 'abs_q05'] = np.quantile(x.abs(), 0.05)
    X_train.loc[segment, 'abs_q95'] = np.quantile(x.abs(), 0.95)
    X_train.loc[segment, 'abs_q99'] = np.quantile(x.abs(), 0.99)
\end{lstlisting}

The results obtained in terms of score and predictions using this set of 24 values entailed another slight improvement, giving a value of 2,097 for the mean absolute error (our score), and fewer negative predictions in \texttt{submission.csv}.

After submitting our results to Kaggle's platform, the score of our kernel calculated by the system was 1,660.

\section[Second model]{Second model}


\section[Third model]{Third model}

